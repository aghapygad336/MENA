{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gtd_happiness.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aghapygad336/MENA/blob/master/gtd_happiness.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdHwAEME_gGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.ensemble import ExtraTreesClassifier, AdaBoostRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import csv\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from openpyxl.utils import dataframe\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import seaborn as sns\n",
        "from math import sqrt\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from matplotlib import colors as cs\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn import preprocessing, metrics\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn import tree\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5VDozn1zbJ9",
        "colab_type": "code",
        "outputId": "8d61f6c0-08f4-4ad6-9c02-1ccb18a75652",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcKYyxdelNPQ",
        "colab_type": "text"
      },
      "source": [
        "**The Global Terrorism Database (GTD)** \n",
        "documents more than 190,000 international and domestic terrorist attacks that occurred worldwide since 1970. With details on various dimensions of each attack, the GTD familiarizes analysts, policymakers, scholars, and journalists with patterns of terrorism. The GTD defines terrorist attacks as: Acts by non-state actors involving the threatened or actual use of illegal force or violence to attain a political, economic, religious, or social goal through fear, coercion, or intimidation. Data collection is ongoing and updates are published annually at."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKXUR64u0CFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "data2015 = pd.read_csv('/content/drive/My Drive/MENA/2015.csv',encoding = \"ISO-8859-1\", engine = 'python')\n",
        "data2016 = pd.read_csv('/content/drive/My Drive/MENA/2016.csv',encoding = \"ISO-8859-1\", engine = 'python')\n",
        "data2017 = pd.read_csv('/content/drive/My Drive/MENA/2017.csv',encoding = \"ISO-8859-1\", engine = 'python')\n",
        "happiness = data2015.append(data2016.append(data2017,sort=True),sort= True)\n",
        "gtd = pd.read_csv('/content/drive/My Drive/MENA/golbal.csv',encoding = \"ISO-8859-1\", engine = 'python', usecols=[1,2,3,8,10,19,20,21,22,26,27,29,35,41,71,84,100,101,103,58])\n",
        "gtd = gtd[(gtd['region_txt'] =='Middle East & North Africa') & (gtd['iyear'] > 2000)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOeLwyrSlt5V",
        "colab_type": "text"
      },
      "source": [
        "**Missing values** are representative of the messiness of real world data. There can be a multitude of reasons why they occur — ranging from human errors during data entry, incorrect sensor readings, to software bugs in the data processing pipeline.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ-hLDHynwqq",
        "colab_type": "text"
      },
      "source": [
        "**Categorial** Let’s start with the most simple thing you can do: removal. As mentioned before, while this is a quick solution, and might work in some cases when the proportion of missing values is relatively low (<10%), most of the time it will make you lose a ton of data. Imagine that just because of missing values in one of your features you have to drop the whole observation, even if the rest of the features are perfectly filled and informative!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZvUPchpn6I1",
        "colab_type": "text"
      },
      "source": [
        "**Numerical NaNs** A standard and often very good approach is to replace the missing values with mean, median or mode. For numerical values you should go with mean, and if there are some outliers try median (since it is much less sensitive to them)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-V8yYdK0w3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gtd missing values\n",
        "numeric_ = gtd._get_numeric_data()\n",
        "cat = gtd.select_dtypes(include='object')\n",
        "mean_nan = numeric_.fillna(numeric_.mean()).dropna(axis=1, how='all')\n",
        "cat_nan = cat.loc[:, cat.isnull().mean() <0.25]\n",
        "gtd = mean_nan.join(cat_nan)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMWTFllUBOxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numeric_ = happiness._get_numeric_data()\n",
        "cat = happiness.select_dtypes(include='object')\n",
        "mean_nan = numeric_.fillna(numeric_.mean()).dropna(axis=1, how='all')\n",
        "cat_nan = cat.loc[:, cat.isnull().mean() <0.25]\n",
        "happiness = mean_nan.join(cat_nan)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-ROtG_qoDsi",
        "colab_type": "text"
      },
      "source": [
        "Better encoding of categorical data can mean better model performance. In this series I’ll introduce you to a wide range of encoding options .\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5YcsrWAfb1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoding(dfData):    \n",
        "    df = dfData.copy().select_dtypes(include='object')\n",
        "    # print(df.columns)\n",
        "    rows,cols = dfData.shape\n",
        "    encodings = {}\n",
        "    for col in df.columns:\n",
        "        Region_nums = []\n",
        "        Region_nums = df[col].unique()\n",
        "        Region_nums = { Region_nums[i]: i for i in range(len(Region_nums)) }\n",
        "        encodings.update({col:Region_nums})\n",
        "        dfData[col].replace(Region_nums, inplace=True)\n",
        "    return dfData,encodings\n",
        "\n",
        "def get_by_value(dic,value):\n",
        "    for key, val in dic.items():\n",
        "        if val == value :\n",
        "            return key    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LeKEcq1fz8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gtd, gtd_encodings = encoding(gtd)\n",
        "happiness, happiness_encodings = encoding(happiness)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9laodI92qfJO",
        "colab_type": "text"
      },
      "source": [
        "Having about 600 terrorist group name (class labels), with some groups with less\n",
        "than 10 attacks which may cause misleading results. You are required to reduce the\n",
        "number of class labels to the 5 groups with max number of attacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HK6ODYye9Jdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_5 = gtd['gname'].value_counts()[1:6]\n",
        "top_5 = [203,27,107,67,58]\n",
        "gtd = gtd[gtd['gname'].isin(top_5)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioH0Hfl4PXbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "combinedData=pd.merge(gtd, happiness, left_on='country_txt', right_on='Country')\n",
        "happiness, happiness_encodings = encoding(happiness)\n",
        "combinedDataE, combinedData_encodings = encoding(combinedData)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTkfx5VXAPRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SplitTrainTes(dataSet):\n",
        "    train_dataSet=dataSet[(dataSet['iyear'] >=2000) & (dataSet['iyear'] <= 2016)]\n",
        "    test_dataSet=dataSet[dataSet['iyear'] ==2017]\n",
        "    return train_dataSet,test_dataSet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A8C6F4gqiYd",
        "colab_type": "text"
      },
      "source": [
        "**Training and Testing**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "a. You are required to use the tuples of years from [2000. 2016] as training set\n",
        "b. Tuples of year 2017 will be used as testing set.\n",
        "c. We have 2 approaches in this assignment\n",
        "i. Train and test using features of GTD only.\n",
        "ii. Train and test using features of GTD and WHR combined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5jPLibb8JAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_GTD,test_GTD=SplitTrainTes(gtd)\n",
        "train_Combined,test_Combined=SplitTrainTes(combinedData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVogRdaoDDFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def splitX_SplitY(trainD,testD):\n",
        "    x_train = trainD.drop(\"gname\", axis=1)\n",
        "    y_train = trainD[\"gname\"]\n",
        "    x_test = testD.drop(\"gname\", axis=1)\n",
        "    y_test = testD[\"gname\"]\n",
        "    return x_train,y_train,x_test,y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayrOo3y6-nlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_trainGTD,y_trainGTD,x_testGTD,y_testGTD=splitX_SplitY(train_GTD,test_GTD)\n",
        "x_trainCombined,y_trainCombined,x_testCombined,y_testCombined=splitX_SplitY(train_Combined,test_Combined)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn7Gay4drqyV",
        "colab_type": "text"
      },
      "source": [
        "**The F measure** \n",
        "\n",
        "---\n",
        "(F1 score or F score) is a measure of a test's accuracy of   **Classifier Models** and is defined as the weighted harmonic mean of the precision and recall of the test. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMGtP-5lEjKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "def Fmeasure(estimator,x_train,y_train,x_test,y_test):\n",
        "    y_pred_train = estimator.predict(x_train)\n",
        "    y_pred_test = estimator.predict(x_test)\n",
        "    print('Train Set Accuracy : ', accuracy_score(y_train, y_pred_train))\n",
        "    print('Train Set Precision : ', precision_score(y_train, y_pred_train, average='micro'))\n",
        "    print('Train Set Recall : ', recall_score(y_train, y_pred_train, average='micro'))\n",
        "    print('Train F-Score for each class : ', f1_score(y_train, y_pred_train, average='micro'))\n",
        "    print('Train Mean F-Score for both classes : ', f1_score(y_train, y_pred_train, average='micro'))\n",
        "\n",
        "    print('----------------------------------------------------------------------')\n",
        "    print('Test Set Accuracy : ', accuracy_score(y_test, y_pred_test))\n",
        "    print('Test Set Precision : ', precision_score(y_test, y_pred_test, average='micro'))\n",
        "    print('Test Set Recall : ', recall_score(y_test, y_pred_test, average='micro'))\n",
        "    print('Test F-Score for each class : ', f1_score(y_test, y_pred_test, average='micro'))\n",
        "    print('Test Mean F-Score for both classes : ', f1_score(y_test, y_pred_test, average='micro'))\n",
        "\n",
        "    print('----------------------------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJKQmfXY-kWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def kNearestNeighborsFunction(x_train,y_train,x_test,y_test):\n",
        "    neighbors = list(range(1, 30, 2))\n",
        "    # empty list that will hold cv scores\n",
        "    cv_scores = []\n",
        "    # perform 10-fold cross validation\n",
        "    for k in neighbors:\n",
        "        knn = KNeighborsClassifier(n_neighbors=k)\n",
        "        scores = cross_val_score(knn, x_train, y_train, cv=10, scoring='accuracy')\n",
        "        cv_scores.append(scores.mean())\n",
        "\n",
        "    mse = [1 - x for x in cv_scores]\n",
        "    optimal_k = neighbors[mse.index(min(mse))]\n",
        "    n=optimal_k\n",
        "    print('Best N found at ' , n)\n",
        "    knn = KNeighborsClassifier(n_neighbors=n)\n",
        "    knn.fit(x_train,y_train)\n",
        "    Fmeasure(knn,x_train,y_train,x_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdQfscD1Dcio",
        "colab_type": "code",
        "outputId": "219c9f47-3734-4542-9093-9aaa4b648e9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        }
      },
      "source": [
        "\n",
        "kNearestNeighborsFunction(x_trainGTD,y_trainGTD,x_testGTD,y_testGTD)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best N found at  3\n",
            "Train Set Accuracy :  0.9203720219136196\n",
            "Train Set Precision :  0.9203720219136196\n",
            "Train Set Recall :  0.9203720219136196\n",
            "Train F-Score for each class :  0.9203720219136196\n",
            "Train Mean F-Score for both classes :  0.9203720219136196\n",
            "----------------------------------------------------------------------\n",
            "Test Set Accuracy :  0.8993329290479078\n",
            "Test Set Precision :  0.8993329290479078\n",
            "Test Set Recall :  0.8993329290479078\n",
            "Test F-Score for each class :  0.8993329290479077\n",
            "Test Mean F-Score for both classes :  0.8993329290479077\n",
            "----------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-AcAuWQmc85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kNearestNeighborsFunction(x_trainCombined,y_trainCombined,x_testCombined,y_testCombined)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BThSmWAL-tu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def DT (x_train,y_train,x_test,y_test):\n",
        "    ct_gini = DecisionTreeClassifier()\n",
        "    ct_entropy = DecisionTreeClassifier(criterion=\"entropy\")\n",
        "    ct_gini = ct_gini.fit(x_train, y_train)\n",
        "    ct_entropy = ct_entropy.fit(x_train, y_train)\n",
        "    Fmeasure(ct_entropy,x_train,y_train,x_test,y_test)\n",
        "    Fmeasure(ct_gini,x_train,y_train,x_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zuya-4z-Du7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DT(x_trainGTD,y_trainGTD,x_testGTD,y_testGTD)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZZXDymbdXJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DT(x_trainCombined,y_trainCombined,x_testCombined,y_testCombined)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7VLD1rP_RLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def NB(x_train,y_train,x_test,y_test):\n",
        "    nb = GaussianNB()\n",
        "    nb = nb.fit(x_train, y_train)\n",
        "    Fmeasure(nb,x_train,y_train,x_test,y_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rexxtAV2ID4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NB(x_trainGTD,y_trainGTD,x_testGTD,y_testGTD)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHnYlvnhdbd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NB(x_trainCombined,y_trainCombined,x_testCombined,y_testCombined)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJroQvOH_-JA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LR(x_train,y_train,x_test,y_test):\n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "    grid = {\"C\": np.logspace(-3, 3, 7), \"penalty\": [\"l1\", \"l2\"]}  # l1 lasso l2 ridge\n",
        "    logreg = LogisticRegression()\n",
        "    logreg_cv = GridSearchCV(logreg, grid, cv=10)\n",
        "    logreg_cv.fit(x_train, y_train)\n",
        "    print(\"tuned hpyerparameters :(best parameters) \", logreg_cv.best_params_)\n",
        "    Fmeasure(logreg_cv,x_train,y_train,x_test,y_test)\n",
        "    print(\"accuracy :\", logreg_cv.best_score_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcjlJjesfcyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LR(x_trainGTD,y_trainGTD,x_testGTD,y_testGTD)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IbmPxNOAFvg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def AB(x_train,y_train,x_test,y_test):\n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "    param_grid = {\n",
        "        'learning_rate': [.1, .2, .3, .4, .5],\n",
        "\n",
        "        'n_estimators': [50, 100, 150, 200, 250]\n",
        "    }\n",
        "\n",
        "    classifier = AdaBoostClassifier()\n",
        "    grid_Search = GridSearchCV(classifier, param_grid=param_grid)\n",
        "    kk = grid_Search.fit(x_train, y_train)\n",
        "    Fmeasure(kk,x_train,y_train,x_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPpRAKjhGrTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AB(x_trainGTD,y_trainGTD,x_testGTD,y_testGTD)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-m5B0xEeKZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AB(x_trainCombined,y_trainCombined,x_testCombined,y_testCombined)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09ila-EbAIRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SVM(x_train,y_train,x_test,y_test):\n",
        "    from sklearn.svm import SVC\n",
        "    svclassifier = SVC(kernel='poly', degree=8)\n",
        "    svclassifier.fit(x_train, y_train)\n",
        "    Fmeasure(svclassifier,x_train,y_train,x_test,y_test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R8AcIciQkAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SVM(x_trainGTD,y_trainGTD,x_testGTD,y_testGTD)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}